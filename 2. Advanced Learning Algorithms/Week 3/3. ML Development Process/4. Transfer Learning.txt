When you dont have a lot of data, transfer learning lets you use data from a different task to help on your application.

For example, you want to classify handwritten digits from 0-9 but dont have enough labeled data for them. So you find a very large dataset of 1 million images of cats, dogs, people etc, upto a 1000 classes. You start training the NN on the data and let it learn to recognize any of the 1000 classes. Suppose the NN has 5 layers including the output layer. So the NN learns the parameters of each layer, you then make a copy of the NN, and copy the weights of the NN you trained with the dataset you found. But you eliminate the last layer, as the dimensions of the output layer will be different, because in the 1 million images dataset, you needed to train the model for 1000 classes, but in the handwritten digit classification problem, you need 10 classes, for digits 0 to 9. Because of this, you train the parameters of the last layer from scratch. There are two options for training the parameters of the NN:

1. You only train the output layer's parameters. You make the parameters of layers 1-4 static, and use stochastic GD or Adam optimizer algorithm to only update the parameters of the output layer.

2. You train all the parameters from layer 1 to output layer. The parameters of layers 1-4 will be initialized using transfer learning.

If you have a very small dataset, option 1 might work little bit better, but if you have slightly larger dataset than option 2 might work a little bit better. This algorithm is called transfer learning, because the intuition is that by learning the images of cats, dogs etc, the NN, hopefully, might have learned some plausible weights for the hidden layers. Then by transfering the parameters to the new NN, the NN starts off at a much better place, so that with just a little bit of furthur learning, we might end up with a good model.

The first step of the transfer learning, the learning of weights through a large dataset of a not-so-related task, is called supervised pre-training. The second step is called fine-tuning,. The model, after learning the weights through a large dataset, is fined tuned to the specific application through a dataset that is made for that application. One good thing about transfer learning is that you dont need to supervised pre-train the model, because for a large number of NNs, there will already be researchers that have trained the NN on a large dataset and will have posted the trained NN on the internet, freely lincensed for anybody to use. You then, just have to replace the output layer and then fine-tune the model using option 1 or 2.

So, the question is, why does transfer learning work? Suppose you are training a NN to detect different objects from images, then the first layer of the NN may learn to detect edges. Then the second layer of the NN may learn to group together edges to detect corners. Then the next layer of the NN may have to detect somewhat complex, but still generic, basic curves or basic shapes. Thats why training the NN on images, you are teaching the NN to detech edges, corners, and basic shapes. That is why training the NN on something diverse like images of cats, dogs and people, you are helping it learn some pretty generic features of images, that is useful for many CV tasks.

One restriction of the pre-training is that the input you will be feeding to the NN to fine-tune it, needs to be of the same type the NN model is pre-trained on. You cant fine-tune a model for audio classification or other audio related problems which is pre-trained on images.