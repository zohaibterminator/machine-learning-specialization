In the example we have discussed so far, each feature could only take on 2 values. But some features can have more than 2 possible values, so we use a method called one-hot encoding for such features.

Now lets say, the ear shape feature can take on an additional value, "oval". So, when you split on this feature, you will create 3 sub-branches of the tree. So instead of using this feature as is, we will instead create 3 more features, called pointy ears, floopy ears and oval ears. If the ear shape of the first example is pointy, we will say that the value of pointy ears feature for this example is 1, and for floppy and oval ears it is 0. So, instead of using a feature that can take on 3 values, we created 3 features that can take on 2 values, 0 or 1. Basically th encoding method is: If a categorical feature can take on k values, create k binary features (0 or 1 valued). If you see the examples, you will see that only 1 of the 3 newly constructed features will be 1, which is why this method is called one-hot encoding. One-hot encoding not only works for decision trees, but can also work when you are training a NN or a logistic regression model for classification.