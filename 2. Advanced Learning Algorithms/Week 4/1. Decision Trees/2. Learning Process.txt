Given a set of 10 training examples, we have to decide what feature to use as the root node. Lets say we picked the Ear shape feature as the root node. What this means now is, we split all of examples according to the value of the feature Ear shape. Lets pick 5 examples of pointy ears and move them to the left, and the five examples of floppy ears to the right. Now, the second step is to just focus on the left part or branch of DT. Lets say you decide to use the face shape feature at the left side. Now we will split the five examples that fall on the left branch, acc to their face shape. For example, we will move the animals with round face shape to the left, and animals with the not round face, to the right. You will notice that, all the examples where the animal was a cat, fell on the left side and all the examples that didnt fall on the right side were dogs. So, we dont need to further divide the branches, and instead we will each make them a seperate leaf node that makes a prediction of what the animal will be if it fell on this side of the DT. Now, we will repeat this process on the right side. If we use the whisker feature as the right decision node, we will then split the examples on the right side based on whether the whiskers are present or absent. You notice that all the examples on the left side of the node (if whiskers are present) are all cats and all the examples on the right side are all dogs, meaning the nodes are pure. So we again, we wont divide the examples further, and make the node on the left side a leaf node that predicts that the example is a cat, and the one on the right predicts that the example is a dog.

Throughout the process, we had to make key decisions at various steps during the algorithm.

Decision 1: How to choose what feature to split on at each node?

At the root node, as well on the left and right decision nodes, we had to decide based on which feature we should split the data to try to maximize purity (or minimize impurity). So, the job of the DT is find which feature results in the maximum purity level ,and select it to split the data based on that feature. Entropy lets us estimate impurity and how to minimize.

Decision 2: When do you stop splitting?

One critera can be that you should stop splitting when a node is 100 percent of one class. Alternatively, you can also stop splittint when splitting a node results in the tree exceeding a specified maximum depth. The depth of a node in DT is defined as the number of hops it takes to reach that node. There can be 2 reasons for you to implement this depth rule One is that it makes the DT less prone to overfitting by keeping it small, and the second is it stops the DT from getting very big and unwieldy. Another stopping criteria can be if the improvements in the node results in minimum improvements to the purity score that is below a certain threshold. One last stopping criteria can be that when a number of examples in a node is below a certain threshold. For example in a particular node, you have 3 nodes, and you decide to stop splitting the examples further, and predict the class that is in the majority.