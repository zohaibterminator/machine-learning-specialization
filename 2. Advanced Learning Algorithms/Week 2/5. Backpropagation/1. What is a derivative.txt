Backprop is a very important algorithm in NN.

Suppose the cost function J(w) is w^2 (ignore b in this example). Let's say the value of the parameter w is equal to 3. J of w will be equal to 9, w squared of 3 squared. Now if we were to increase w by a tiny amount, say Epsilon, which I'm going to set to 0.001. How does the value of J of w change? If we increase w by 0.001 then w becomes 3 plus 0.001, so it's 3.001. J of w, which is w squared, which we defined above, is now this 3.001 squared, which is 9.006001. What we see is that if w goes up by 0.001, I'm going to use this up arrow here to denote w goes up by 0.001, where 0.001 is this small value Epsilon. Then J of w roughly goes up by 6 times as much, 6 times 0.001. This isn't quite exactly. It actually goes up not to 9.006 but 9.006001. But it turns out that if Epsilon where infinitesimally small, and by infinitesimally small I mean very small. Epsilon is pretty small, but it's not infinitesimally small. If Epsilon was 0.00000, lots of zeros followed by one, then this becomes more and more accurate. In this example what we see is that if w goes up by Epsilon then J goes up roughly by 6 times Epsilon. In calculus what we would say is that the derivative of J of w with respect to w is equal to 6. All this means is if w goes up by a tiny little amount J of w goes up six times as much. What if Epsilon were to take on different value? What if Epsilon were 0.002. In this case w would be 3 plus 0.002, and w squared becomes 3.002 squared, which is 9.012004.
In this case what we conclude is that if w goes up by 0.002 then J of w goes up by roughly 6 times 0.002. It goes up roughly to 9.012, and this 0.012 is roughly 6 times 0.002. That again, is a little bit off. This is extra 0.00004 here, because Epsilon is not quite infinitesimally small. Once again, we see this six to one ratio between how much w goes up versus how much J of w goes up. That's why the derivative of J of w with respect to w is equal to six.

This leads to an informal definition of derivative, which is:

if w goes up by epsilon, then J(w) goes up by k * epsilon.

d/dw (J(w)) = k

In our example, k was 6.

You might remember when implementing gradient descent you will repeatedly use this rule to update the parameter w_j, where as usual Alpha is the learning rate. What is gradient descent do? Notice that if the derivative is small, then this updates that will make a small update to the parameter W_j, whereas if this derivative term is large, this will result in a big change to the parameter W_j.This makes sense because this is essentially saying that if the derivative is small, this means that changing w doesn't make a big difference to the value of j and so let's not bother to make a huge change to W_j. But if the derivative is large, that means that even a tiny change the W_j can make a big difference in how much you can change or decrease the cost function j of w. In that case, let's make a bigger change to W_j, because doing so will actually make a big difference to how much we can reduce the cost function J.