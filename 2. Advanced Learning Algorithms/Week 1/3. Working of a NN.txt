Example, Demand prediction.
Look at the product, and try to predict whether this product will be a top seller or not. This type of application of NN is used by retailers today in order to plan better inventory levels as well as marketing campaigns. The product we will discuss for the example is t-shirts.

Input x is the price of the t-shirt. We will first use the logistic regression to classify the t-shirt. The equation will be written as:

f(x) = 1/(1 + e^-(wx + b))

Previously, we used to denote the output of the equation as f(x). In order to set us up to build a neural network, we are going to switch the terminology a little bit and use the alphabet 'a' to denote the output of this logistic regression algorithm. 

a = f(x) = 1/(1 + e^-(wx + b))

The 'a' refers to activation, which is a terminologyin neuroscience and it refers to how much a neuron is sending a high output to other neurons downstream from it. This logistic regression equation can be thought of a very simplified model of the neuron of the brain. The neuron takes the input x, which is the price of the t-shirt, and puts it into the logistic regression equation, and outputs the value 'a', which is the probability of the t-shirt being a top-seller. Another way to think of a neuron is as a tiny little computer whose only job is to input one number or a few numbers, such as a price, and then to output one number or maybe a few other numbers which in this case is the probability of the T-shirt being a top seller. Given this description of the neuron, building a neural network now requires just connecting a bunch of neurons.

Now we will take a more complex example of the demand prediction. Now there are four features, price, shipping cost, marketing cost and material. Now, you might suspect that whether or not a T-shirt becomes a top seller actually depends on a few factors. First, one is the affordability of this T-shirt. Second is, what's the degree of awareness of this T-shirt that potential buyers have? Third is perceived quality to bias or potential bias saying this is a high-quality T-shirt. What I'm going to do is create one artificial neuron to try to estimate the probability that this T-shirt is perceive as highly affordable. Affordability is mainly a function of price and shipping costs because the total amount of the pay is some of the price plus the shipping costs. We're going to use a little neuron here, a logistic regression unit to input price and shipping costs and predict do people think this is affordable? Second, I'm going to create another artificial neuron here to estimate, is there high awareness of this? Awareness in this case is mainly a function of the marketing of the T-shirt. Finally, going to create another neuron to estimate do people perceive this to be of high quality, and that may mainly be a function of the price of the T-shirt and of the material quality. Price is a factor here because fortunately or unfortunately, if there's a very high priced T-shirt, people will sometimes perceive that to be of high quality because it is very expensive than maybe people think it's going to be of high-quality. Given these estimates of affordability, awareness, and perceived quality we then wire the outputs of these three neurons to another neuron here on the right, that then there's another logistic regression unit. That finally inputs those three numbers and outputs the probability of this t-shirt being a top seller. We will group the first three neurons together into what's called a layer.

A layer is a grouping of neurons which takes as input the same or similar features, and in turn outputs a few numbers together. The three neurons form one layer. And the neuron that takes the output of the three neurons also forms one layer. A layer can have multiple neurons, or just one neuron. The seconf layer, the one with the one neuron is also called the output layer, as it gives the final output, the probability of the shirt being a top seller. In the terminology of neural networks, we're also going to call affordability, awareness and perceived quality to be activations. The activations of the neurons of the first layer are affordability, awareness and perceived quality while the activation of the output layer is the probability.

The NN we discussed work in the following order, it first takes 4 inputs, performs some computations and outputs 3 numbers of activations, then output layer takes those activation as input and performs some computations and output another activation as the final output.

In a neural network this list of four numbers is also called the input layer, and that's just a list of four numbers. Now, there's one simplification I'd like make to this neural network. The way I've described it so far, we had to go through the neurons one at a time and decide what inputs it would take from the previous layer. For example, we said affordability is a function of just price and shipping costs and awareness is a function of just marketing and so on, but if you're building a large neural network it'd be a lot of work to go through and manually decide which neurons should take which features as inputs. The way a neural network is implemented in practice each neuron in a certain layer; say this layer in the middle, will have access to every feature, to every value from the previous layer, from the input layer which is why I'm now drawing arrows from every input feature to every one of these neurons shown here in the middle. You can imagine that if you're trying to predict affordability and it knows what's the price shipping cost marketing and material, may be you'll learn to ignore marketing and material and just figure out through setting the parameters appropriately to only focus on the subset of features that are most relevant to affordability. To further simplify the notation and the description of this neural network I'm going to take these four input features and write them as a vector x, and we're going to view the neural network as having four features that comprise this feature vector x. This feature vector is fed to this layer in the middle which then computes three activation values. That is these numbers and these three activation values in turn becomes another vector which is fed to this final output layer that finally outputs the probability of this t-shirt to being a top seller. That's all a neural network is. It has a few layers where each layer inputs a vector and outputs another vector of numbers. For example, this layer in the middle inputs four numbers x and outputs three numbers corresponding to affordability, awareness, and perceived quality. To add a little bit more terminology, you've seen that this layer is called the output layer and this layer is called the input layer. To give the layer in the middle a name as well, this layer in the middle is called a hidden layer. I know that this is maybe not the best or the most intuitive name but that terminology comes from that's when you have a training set. In a training set, you get to observe both x and y. Your data set tells you what is x and what is y, and so you get data that tells you what are the correct inputs and the correct outputs. But your dataset doesn't tell you what are the correct values for affordability, awareness, and perceived quality. The correct values for those are hidden. You don't see them in the training set, which is why this layer in the middle is called a hidden layer. I'd like to share with you another way of thinking about neural networks that I've found useful for building my intuition about it. Just let me cover up the left half of this diagram, and see what we're left with. What you see here is that there is a logistic regression algorithm or logistic regression unit that is taking as input, affordability, awareness, and perceived quality of a t-shirt, and using these three features to estimate the probability of the t-shirt being a top seller. This is just logistic regression. But the cool thing about this is rather than using the original features, price, shipping cost, marketing, and so on, is using maybe better set of features, affordability, awareness, and perceived quality, that are hopefully more predictive of whether or not this t-shirt will be a top seller. One way to think of this neural network is, just logistic regression. But as a version of logistic regression, they can learn its own features that makes it easier to make accurate predictions. In fact, you might remember from the previous course, this housing example where we said that if you want to predict the price of the house, you might take the frontage or the width of lots and multiply that by the depth of a lot to construct a more complex feature, x_1 times x_2, which was the size of the lawn. There we were doing manual feature engineering where we had to look at the features x_1 and x_2 and decide by hand how to combine them together to come up with better features. What the neural network does is instead of you needing to manually engineer the features, it can learn, as you'll see later, its own features to make the learning problem easier for itself. This is what makes neural networks one of the most powerful learning algorithms in the world today. To summarize, a neural network, does this, the input layer has a vector of features, four numbers in this example, it is input to the hidden layer, which outputs three numbers. I'm going to use a vector to denote this vector of activations that this hidden layer outputs. Then the output layer takes its input to three numbers and outputs one number, which would be the final activation, or the final prediction of the neural network.

In practice, you dont need to explicitly define the features that it needs to compute like affordability etc, the NN will decide for itself what are the features that it needs to use in the hidden layer.

One decision you will have to make is how many hidden you want and how many neurons you want in each hidden layer. A neural network with multiple hidden layers is sometimes referred as "multilayer perceptron".