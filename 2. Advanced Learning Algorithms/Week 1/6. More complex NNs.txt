We will use an example of NN with 4 layers. Layer 1-3 are the hidden layers and layer 4 is the output layer. There is also an input layer which will be denoted as layer 0. When we saw, a NN has 4 layers, it includes all the hidden layers and output layers but not the input layer.

We will look at the workings of the hidden layer, layer 3. The layer takes in a^[2] as the input, which is the activation of the layer before it, layer 2. If it has three neurons, or three hidden units, it has the parameters w_1^[3] and b_1^[3], w_2^[3] and b_2^[3] and w_3^[3] and b_3^[3], and it computes activations:

a_1^[3] = g(w_1^[3] . a^[2] + b_1^[3])
a_2^[3] = g(w_2^[3] . a^[2] + b_2^[3]) 
a_3^[3] = g(w_3^[3] . a^[2] + b_3^[3]) 

The output of the layer will be a vector a^[3] comprising of values [a_1^[3], a_2^[3], a_3^[3]]

The general function of the logistic regression in a neuron or unit 'j' in layer 'l':

a_j^[l] = g(w_j^[l] . a^[l-1] + b_j^[l]) 

The sigmoid function in the neuron is called activation function, as it outputs the activation value of the neuron. There are other activation function besides this function.