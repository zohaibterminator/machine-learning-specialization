Take the example of house price prediction.

Consider a graph of a dataset with size on x-axis and price on y-axis. The points on the graph increase at first but flatten out towards the end, meaning, at a certain point, as the size of house increases, the price of the house doesnt increase at the same rate. Lets apply the linear regression model "w_1 * x + b" which uses just one feature to learn and predict the price of the house. But after applying a linear regression model, the model just makes a straight, linear line which doesnt fit the data well. This is called "Underfitting" or the model has "high bias". This means that the there is a pattern in the dataset that the algorithm just didnt seem to capture due to some preconceved bias. Lets consider the other extreme. Lets take a polynomial linear regression model:
    w_1 * x + w_2 * x^2 + w_3 * x^3 + w_4 * x^4 + b

This gives a line which perfectly passes all the points exactly. There will be no error, because the curve passes all the points. This model, although, will give very good results during testing, will not generalize to new data given to it. This is called "Overfitting" or the model has "high variance". If this model is trained by two engineers with one engineer having some new data with them, they could end up with two very different predictions or highly variable predictions (hence called high variance). A modek shouldn't underfit or overfit, i.e. not have high bias or high variance. This concept also applies to classification models.