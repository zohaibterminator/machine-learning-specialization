If your learning curve goes up and down or it is constantly increasing, it is a clear sign that GD is not working properly. Either there is a bug in the code or the learning rate is too large. Usually, if the cost incresing or fluctuating, set the value of Alpha to a very small number to debug, the code. If it still exhibits the same behavior then there is a bug in the code. One useful tip for choosing the learning rate is try a range of values. If you choose 0.001, try 0.01 next for a few iterations and see the learning curve. you should choose the learning curve which reduces the cost rapidly but consistently.