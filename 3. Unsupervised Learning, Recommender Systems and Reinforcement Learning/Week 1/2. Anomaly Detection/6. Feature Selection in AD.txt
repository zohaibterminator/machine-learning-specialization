Choosing good choice of features is really important.

One thing you can do is to make your features Gaussian (have normal dist)., if they are not Gaussian, you can make them Gaussian. You can plot the distribution of features using a histogram. For example, if a feature looks a little skewed, you can transform it into a Gaussian dist by applying log function to the values of the features. Another way of transforming, lets say, feature x_1, you can use x_1 = log(x_1 + 1) or x_1 = log(x_1 + C), where C is constant, as another transforming technique. Some of the other transformation techniques you can apply are:

x = sqrt(x)
x = x ^ 1/3 (take cube root)

It turns out, a larger value of C will transform the distribution less, but you can try lots of different values of C and pick one that make the distribution look better.

When applying these transformation, make sure to also apply these to your test set and validation set as well.

You can also apply error analysis, if your algorithm is still not doing well on the validation set. you can try to look at where the algorithm is not yet doing well whereas making errors, and then use that to try to come up with improvements. What do you normall want is you want the P(x) to be large, or P(x) >= epsilon, for normal examples and P(x) to be small, or P(x) < epsilon.

One of the most common problems you find in your algorithm is that it will be giving you comparable values for P(x) for both the normal data and anomalous data. For example, if you find an example that should be anomalous, but the algorithms did not flag it as anomalous, you will see what is the difference between this example and other similar examples. If you find a feature that distinguishes the anomalous example from a normal example, by including that feature, you will greatly improve the performance of your algorithm.

Just as one more example, let's say you're building an anomaly detection system to monitor computers in the data center. To try to figure out if a computer may be behaving strangely and deserves a closer look, maybe because of a hardware failure, or because it's been hacked into or something. So what you'd like to do is, to choose features that might take on unusually large or small values in the event of an anomaly. You might start off with features like X one is the memory use, X two is the number of disk accesses per second, then the CPU load, and the volume of network traffic. And if you train the algorithm, you may find that it detects some anomalies but fails to detect some other anomalies. In that case, it's not unusual to create new features by combining old features. So, for example, if you find that there's a computer that is behaving very strangely, but neither is CPU load nor network traffic is that unusual. But what is unusual is, it has a really high CPU load, while having a very low network traffic volume. If you're running the data center that streams videos, then computers may have high CPU load and high network traffic, or low CPU load and no network traffic. But what's unusual about this one machine is a very high CPU load, despite a very low traffic volume. In that case, you might create a new feature X five, which is a ratio of CPU load to network traffic. And this new feature with hope, the anomaly detection algorithm flagged future examples like the specific machine you may be seeing as anomalous. Or you can also consider other features like the square of the CPU load, divided by the network traffic volume. And you can play around with different choices of these features. In order to try to get it so that P of X is still large for the normal examples but it becomes small in the anomalies in your cross validation set.