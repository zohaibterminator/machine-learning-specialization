When developing a learning algorithm (choosing features, increasing/decreasing epsilon and changing some other parameters), making decision is much easier if you have a way of evaluating it. This is sometimes called Real-number evaluation, meaning if you change a parameter in your algorithm and you have a way for calculating a number that tells you whether the algorithm got better or worse, then it is much easier for you to decide whether to stick with that change. 

This is how it is often done in anomaly detection. Although we have been mainly talking about unlabeled data, we will assume we have small number of anomalous and non-anomalous examples. We will assume for the labelled data that is anomalous, label y will be 1, and 0 otherwise. The unlabelled training examples that the algorithm will learn, we will assume that the label for all the examples will be 0 (even if a few anomalous examples sneak in the set, the algorithm will still usually work fine). To evaluate a model, a small amount of anomalous examples will be beneficial. We will then create a cross-validation set and a test set, where both the sets hopefully have some number of anomalous examples. The algorithm will work fine even if an example was given to it that was anomalous, but was accidently labelled 0.

Let's say you have been manufacturing aircraft engines for years and so you've collected data from 10,000 good or normal engines, but over the years you had also collected data from 20 flawed or anomalous engines. Usually the number of anomalous engines, that is y equals 1, will be much smaller. It will not be a typical to apply this type of algorithm with anywhere from, say, 2-50 known anomalies. We're going to take this dataset and break it up into a training set, a cross validation set, and the test set. Here's one example. I'm going to put 6,000 good engines into the training set. Again, if there are couple of anomalous engines that got slipped into this set is actually okay, I wouldn't worry too much about that. Then let's put 2,000 good engines and 10 of the known anomalies into the cross-validation set, and a separate 2,000 good and 10 anomalous engines into the test set. What you can do then is train the algorithm on the training set, fit the Gaussian distributions to these 6,000 examples and then on the cross-validation set, you can see how many of the anomalous engines it correctly flags. For example, you could use the cross validation set to tune the parameter epsilon and set it higher or lower depending on whether the algorithm seems to be reliably detecting these 10 anomalies without taking too many of these 2,000 good engines and flagging them as anomalies. After you have tuned the parameter epsilon and maybe also added or subtracted or tuned to features X_J you can then take the algorithm and evaluate it on your test set to see how many of these 10 anomalous engines it finds, as well as how many mistakes it makes by flagging the good engines as anomalous ones. Notice that this is still primarily an unsupervised learning algorithm because the training sets really has no labels or they all have labels that we're assuming to be y equals 0 and so we learned from the training set by fitting the Gaussian distributions as you saw in the previous video. But it turns out if you're building a practical anomaly detection system, having a small number of anomalies to use to evaluate the algorithm that your cross validation and test sets is very helpful for tuning the algorithm.

Since the number of anomalies are so small, one alternative can be is that you use a training set for training the algorithm, then only use a cross-validation set to evaluate the algorithm, without a test set. So, in the aircraft example, if you only had 2 examples that were labelled an anomaly, you would put all of them in the cross-validation set, and not use a test set.