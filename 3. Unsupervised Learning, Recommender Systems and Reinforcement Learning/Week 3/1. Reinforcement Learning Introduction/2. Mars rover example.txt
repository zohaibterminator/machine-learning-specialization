We will use a simpler example inspired by mars rover. In this example, the rover can be in any of the six positions showed in the form of six boxes. The mars rover can be at the position 4, or box 4. This position of the Mars rover is called the state in RL, and we will name each box a different state like box 1 be called state 1, box 2 be called state 2 and so on. The rover is sent to Mars to carry out different science missions. It can go to different places to use it's sensors such as a drill, or a radar, or a spectrometer to analyze the rock at different places on the planet, or go to different places to take different pictures for the scientists on Earth to look at.

In this example, the state 1 has a interesting surface that the scientist would love the rover to sample. There is also another interesting surface the scientists would like the robot to sample as well as the state 6, but it is not as interesting as state 1. So, we would more like to carry out the science mission at state 1 than at state 6, but state 1 is furthur away. The way we will reflect state 1 being potentially more valuable is through the reward function. The reward at stage 1 is 100, and the reward at stage 2 is 40. The reward for all the states in between is 0, because there aren't any interesting science missions we can carry out in them. The rover can either go left or go right. So, what should the rover do? In RL, we pay a lot of attention to the rewards, because thats how we know if the robot is doing well or poorly. If the robot was to go left, starting from state 4, then initially, it will get 0 reward in the state 4, then it will go left to the state 3, it will still get 0 reward, and the same at reward of 0. Finally, when it gets to state 1, it will recieve a reward of 100. We are going to assume for this example, when the rover gets to either state 1 or state 6, that the day ends. In RL, we sometimes call it terminal state, meaning after the robot gets to a terminal stage and gets it's reward, nothing happens after that. Maybe the robots run out fuel or run out of time for the day, meaning it gets to enjoy either 100 reward or 40 reward and thats it for the day.

Now, if the robot chooses to go right, it will have a reward of 0, then it will go to state 5, again will get no rewards, and then go right to state 6 or the other terminal state and get a reward of 40. Now, the robot can't only go right or only go left, it can go to the right first from state 4 having 0 rewards, and reach state 5 and recieve 0 reward, then change it's mind and then start to go left to state 4, then 3, then 2 and then finally state 1 for the reward of 100. In this sequence of actions, the robot is wasting a lot of time, so this isn't such a great way to take actions, but it is a choice an algorithm could pick.

So, at any point in time, the robot is in a state s, it chooses an action a, and it also gets to enjoy reward R(s) of that state. And as a result of the action it gets to some new state, s'. These are the core elements to look at when deciding how to take actions.

(s, a, R(s), s')