Lets take an example of features X_1 and X_2. PCA algorithm is an unsupervised algorithm so there is no label y. Before applying steps for PCA algorithm, we will make sure our algorithm is mean normalized (x = (x - mean)/(max_val - min_val)), and the features are also scaled. Lets say we take the x-axis as the "z-axis". We will then "project" it down to the z-axis. The word project refers to that you're taking this example and bringing it to the z-axis. If you see the image of example 1, you will see that the z-axis is capturing a lot of the variance of the features, but we can do better. In the example 2, the variance is less represented on the z-axis. So this is a bad choice of the axis. Lets look at 1 last example, example 3. This example is much better choice, because we can see that the points are reflected as far apart, which captures more info. This axis is called the "principle component" in PCA, because it is an axis that captures the maximum amount of variance.

Lets say the PCA algo has choosen an axis, how will it project the data. Lets go back to our X_1 and X_2 example. Lets say the co-ordinates for X_1 and X_2 is (2, 3). The PCA algo has found a vector of length 1, which is represented by the co-ordinates (0.71, 0.71). So, the way we will project the co-ordinate (2, 3) will be projected is that we will take a dot-product of the co-ordinates (2, 3) and (0.71, 0.71), which turns out to be 3.55 . This means the distance of this point from the origin is 3.55 . Now this number will be used to represent the co-ordinates (2, 3) of X_1 and X_2. This axis can also be called first principle component. If you will pick up a 2nd axis, the 2nd principle component will always be at 90 degrees to the first prinicple axis or be "perpendicular" to the 1st axis. Now, what if you want the original co-ordinates the 3.55 number represents. There is a way that you can approximately recalculate the original co-ordinates. There is a concept in PCA called reconstruction, that does this exact thing. So to calculate the original co-ordinates of 3.55, you will multiple it with the length 1 vector (0.71, 0.71). As a result you get the co-ordinates (2.52, 2.52), which approximately represents the original co-ordinates. The difference between the original point and this approximated point is the length of the projection caused by the original point onto the z-axis.

How is LR different to PCA? Well, one difference is that the LR is a supervised learning model, meaning we are trying to fit a straight line that minimizes the distance along the y-axis (if you plot the feature x and y on a graph). Meanwhile in PCA, we are not minimizing the distance of any feature to the z-axis. Linear regression is used to predict a target output Y and PCA is trying to take a lot of features and treat them all equally and reduce the number of axis needed to represent the data well.