Lets look at what you can do if you dont have access to the features of each items in advance and how you can come up with them using the data.

Lets say you dont have the values for features x_1 and x_2, but you have parameters for the linear regression you used to predict the ratings for all the users, you can use it to calculate the feature vector x^(1) for movie 1, x^(2) for movie 2, etc. But this will not work if you dont have the parameters for the users already, which is why in normal linear regression applications, you can't come up with new features because you don't have the parameters for the model. But in collaborative filtering you can do this because you have multiple ratings for the same movie, which makes it possible. So, the cost function will become:

j(x^(i)) = 1/2 * sum((w^(j) * x^(i) + b^(j) - y^(i, j))^2)

For all users j that have rated the movie i (i.e. for all r^(i, j))

And the regularized cost function will be:

j(x^(i)) = 1/2 * sum((w^(j) * x^(i) + b^(j) - y^(i, j))^2) + lambda/2 * sum(x_k^(i) ** 2)

Where k in x_k means all features 1 through n.

The two algorithms we have learned are combinedly called Collaborative filtering. When combined the cost function becomes a function of 3 parameters, w, b and x. If we want to minimize the cost function using GD, we also update the parameter x_k^(i). The algorithm Collaborative filtering is called as such because multiple users collaboratively have given you ratings for movies, though which you can calculate the features for the movies, which you can then use to predict the ratings of the movies the users has not given rating to.

We have been using ratings from 0-5 or 1-5, but a common use case for collaborative filtering uses binary labels like a users favors or likes or interact with a particular item.