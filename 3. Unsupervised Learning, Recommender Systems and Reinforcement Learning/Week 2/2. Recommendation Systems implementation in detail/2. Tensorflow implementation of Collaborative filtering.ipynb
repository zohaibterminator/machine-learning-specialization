{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Tensorflow for implementing Recommender Systems?\n",
    "\n",
    "Tensorflow allows you to calculate derivatives for the cost functions, so you can calculate the derivative of the cost function without even knowing any calculas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of calculating a derivative of a Cost Function\n",
    "\n",
    "We will look at an example of a cost function:\n",
    "\n",
    "J = (w.x - y) ** 2\n",
    "\n",
    "The value for y and x will be 1, and we will initialise the value of w as 3. We will be finding the derivative of cost function and updating the value of w with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(3.0)\n",
    "x = y = 1.0\n",
    "\n",
    "alpha = 0.1\n",
    "epochs = 30\n",
    "\n",
    "for iter in range(epochs):\n",
    "    \n",
    "    # To enable automatic differentiation, we need to use tf.GradientTape(), and tell the 'tape' how costJ is calculated, so that it can calculate the gradient of costJ with respect to w\n",
    "    with tf.GradientTape() as tape:\n",
    "        f_wb = w*x\n",
    "        costJ = (f_wb - y) ** 2\n",
    "    \n",
    "    # Use the tape to calculate the gradient of costJ with respect to w\n",
    "    [dJ_dW] = tape.gradient(costJ, [w])\n",
    "    \n",
    "    # Update the value of w\n",
    "    w.assign_add(-alpha * dJ_dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.002476\n"
     ]
    }
   ],
   "source": [
    "print(w.numpy()) # Value of w after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Differentiation is also called Auto Diff, but it is also called Auto Grad, but the technically correct term is Auto Diff, because Auto Grad is the name of a software that calculates Auto Diff. You can use Auto Diff not just for implementing Gradient Descent, but also Adam optimizer in TensorFlow as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of calculation of Derivatives and Adam optimizer for Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimizer with learning rate 0.1\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-1)\n",
    "\n",
    "epochs = 200\n",
    "for iter in range(epoch);\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost_value = tf.cofiCostFuncV(X, W, b, Ynorm, R, num_users, num_movies, lambda_reg) # define the collaborative filtering cost function\n",
    "\n",
    "    grads = tape.gradient(cost_value, [X, W, b]) # calculate the gradients of the cost function with respect to X, W, and b\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, [X, W, b])) # update the values of X, W, and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, if you are asking why we didn't fit the data into a Dense layer of a NN and train the parameters that way, it's because the data can't be fit into a Dense layer, so we instead use Auto Diff in this way to calculate the derivatives of the cost function and implement the algorithm that way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_mentorship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
